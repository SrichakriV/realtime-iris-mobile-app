<!DOCTYPE html>
<html>
<head>
    <title>Iris Seg - Mobile</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <style>
        body { margin: 0; background: #000; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; overflow: hidden; }
        /* Container ensures the canvas overlays the video exactly */
        #container { position: relative; width: 100%; max-width: 600px; aspect-ratio: 3/4; }
        video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); /* Mirror for selfie mode */ }
        #status { position: absolute; top: 10px; left: 10px; color: lime; font-family: monospace; z-index: 10; background: rgba(0,0,0,0.5); padding: 5px; }
    </style>
</head>
<body>

<div id="container">
    <div id="status">Loading Model...</div>
    <video id="webcam" autoplay playsinline></video>
    <canvas id="output"></canvas>
</div>

<script>
    const MODEL_PATH = './model.onnx'; // Make sure this file is in the same folder
    let session = null;

    // Elements
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('output');
    const ctx = canvas.getContext('2d');
    const status = document.getElementById('status');

    async function start() {
        try {
            // 1. Load Model
            status.innerText = "Downloading Model...";
            session = await ort.InferenceSession.create(MODEL_PATH, { executionProviders: ['wasm'] });
            status.innerText = "Model Ready. Starting Cam...";

            // 2. Start Camera (Request Selfie Camera)
            const stream = await navigator.mediaDevices.getUserMedia({
                video: { facingMode: "user", width: { ideal: 640 }, height: { ideal: 480 } },
                audio: false
            });
            video.srcObject = stream;

            // 3. Wait for video to play
            video.onloadeddata = () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                requestAnimationFrame(runInference);
            };

        } catch (e) {
            status.innerText = "Error: " + e;
            console.error(e);
        }
    }

    async function runInference() {
        if (!session || video.paused || video.ended) return;

        // --- PRE-PROCESSING ---
        // Draw video to a temp canvas to get pixel data
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
        const { data, width, height } = imageData;

        // Resize to 224x224 (Model Input) manually or use a helper
        // For simplicity, we'll do a center crop/resize simulation here or 
        // just pass the raw data if we implement a resize loop. 
        // To keep this "Simple" and copy-pasteable, we will use a raw resize approach:
        
        // Prepare Float32 Input Tensor [1, 3, 224, 224]
        const inputSize = 224;
        const floatArr = new Float32Array(1 * 3 * inputSize * inputSize);
        
        // Simple Nearest Neighbor Resize & Normalize loop
        const xRatio = width / inputSize;
        const yRatio = height / inputSize;

        for (let y = 0; y < inputSize; y++) {
            for (let x = 0; x < inputSize; x++) {
                const srcX = Math.floor(x * xRatio);
                const srcY = Math.floor(y * yRatio);
                const srcIdx = (srcY * width + srcX) * 4;

                const r = data[srcIdx];
                const g = data[srcIdx + 1];
                const b = data[srcIdx + 2];

                // Normalize (0-1) and Standardize (mean=0.5, std=0.5) => (x - 0.5)/0.5 = 2*x - 1
                // Index mapping for NCHW format
                const i = (y * inputSize + x);
                floatArr[i] = (r / 255.0 - 0.5) / 0.5; // Red channel
                floatArr[i + (inputSize * inputSize)] = (g / 255.0 - 0.5) / 0.5; // Green
                floatArr[i + (2 * inputSize * inputSize)] = (b / 255.0 - 0.5) / 0.5; // Blue
            }
        }

        const tensor = new ort.Tensor('float32', floatArr, [1, 3, inputSize, inputSize]);

        // --- INFERENCE ---
        const startT = performance.now();
        const feeds = { input: tensor }; // 'input' must match the ONNX export name
        const results = await session.run(feeds);
        const output = results.output.data; // 'output' must match ONNX export name
        const endT = performance.now();
        
        status.innerText = `Inference: ${Math.round(endT - startT)}ms`;

        // --- POST-PROCESSING ---
        // Overlay the mask
        // Output is 1x1x224x224. We need to scale it back to canvas size.
        
        const maskImg = ctx.createImageData(width, height);
        const maskData = maskImg.data; // This is a 1D array RGBA

        for (let y = 0; y < height; y++) {
            for (let x = 0; x < width; x++) {
                // Map back to 224x224
                const modelX = Math.floor(x / xRatio);
                const modelY = Math.floor(y / yRatio);
                const modelIdx = modelY * inputSize + modelX;
                
                const val = output[modelIdx]; // Sigmoid output (0 to 1)

                // If prediction > 0.5, draw Purple
                const pixelIdx = (y * width + x) * 4;
                if (val > 0.5) {
                    maskData[pixelIdx] = 128;     // R
                    maskData[pixelIdx + 1] = 0;   // G
                    maskData[pixelIdx + 2] = 128; // B
                    maskData[pixelIdx + 3] = 150; // Alpha (Transparency)
                } else {
                    maskData[pixelIdx + 3] = 0;   // Transparent
                }
            }
        }
        
        ctx.putImageData(maskImg, 0, 0);

        requestAnimationFrame(runInference);
    }

    start();
</script>

</body>
</html>